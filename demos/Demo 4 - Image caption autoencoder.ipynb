{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo 4 - Image caption autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=13.19s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.89s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "import mycoco\n",
    "mycoco.setmode(\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "Let's get all the captions from COCO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "allcaptions = mycoco.query([['']])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118287"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(allcaptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = mycoco.get_captions_for_ids(allcaptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A subway station above groundtrain with a small green building.',\n",
       " 'A very long elevated train is making its way through the city.',\n",
       " 'A photo of four different train tracks and a train on one of them.',\n",
       " 'A big train riding along on the train tracks',\n",
       " 'The train is going down the railroad tracks. ',\n",
       " 'A row of surfboards that are lined up on a beach.',\n",
       " 'A bunch of surfboards leaning on cement in the sand.',\n",
       " 'A group of surfboards sitting on top of a sandy beach.',\n",
       " 'Two people sitting between surfboards propped up at the beach.',\n",
       " 'Surfboards are line up against a parking lot.',\n",
       " 'SOMEONE IS SKIING AND JUMPING HIGH OFF A MOUNTAIN',\n",
       " 'A skier is flying in the air in the middle of a mountainous area.',\n",
       " 'A person going down a snowy hill with skis.',\n",
       " 'A skier is taking a large jump on a slope',\n",
       " 'A photograph of a skier performing a stunt.',\n",
       " 'A mouse sitting on book, with a Microsoft logo on it.',\n",
       " 'THIS IS A PHOTO OF A MOUSE ON TOP OF A BOOK',\n",
       " 'A computer mouse is sitting on the page of a book.',\n",
       " 'this is a mouse on a page on a table',\n",
       " 'A mouse sitting on a book under an illustration of a tree.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions[50000:50020]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we tokenize the sentences and convert them to lists of word indices (as integers), which we need to train neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer, one_hot\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 751, 220, 234, 6, 1, 34, 63, 67],\n",
       " [1, 138, 238, 1950, 40, 8, 458, 150, 400, 104, 4, 81],\n",
       " [1, 166, 3, 227, 188, 40, 196, 7, 1, 40, 2, 101, 3, 246],\n",
       " [1, 163, 40, 44, 218, 2, 4, 40, 196],\n",
       " [4, 40, 8, 271, 29, 4, 642, 196],\n",
       " [1, 406, 3, 507, 24, 17, 417, 32, 2, 1, 72],\n",
       " [1, 168, 3, 507, 457, 2, 747, 5, 4, 408],\n",
       " [1, 31, 3, 507, 11, 2, 30, 3, 1, 563, 72],\n",
       " [13, 16, 11, 410, 507, 1587, 32, 14, 4, 72],\n",
       " [507, 17, 360, 32, 313, 1, 199, 194],\n",
       " [362, 8, 243, 7, 283, 355, 211, 1, 256],\n",
       " [1, 353, 8, 83, 5, 4, 120, 5, 4, 216, 3, 1, 2289, 99],\n",
       " [1, 27, 271, 29, 1, 244, 222, 6, 156],\n",
       " [1, 353, 8, 225, 1, 25, 607, 2, 1, 230],\n",
       " [1, 453, 3, 1, 353, 618, 1, 1213],\n",
       " [1, 426, 11, 2, 374, 6, 1, 7635, 2161, 2, 26],\n",
       " [137, 8, 1, 166, 3, 1, 426, 2, 30, 3, 1, 374],\n",
       " [1, 111, 426, 8, 11, 2, 4, 2541, 3, 1, 374],\n",
       " [137, 8, 1, 426, 2, 1, 2541, 2, 1, 22],\n",
       " [1, 426, 11, 2, 1, 374, 142, 12, 5312, 3, 1, 130]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[50000:50020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences2 = [[0] + x + [0] for x in sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 751, 220, 234, 6, 1, 34, 63, 67, 0],\n",
       " [0, 1, 138, 238, 1950, 40, 8, 458, 150, 400, 104, 4, 81, 0],\n",
       " [0, 1, 166, 3, 227, 188, 40, 196, 7, 1, 40, 2, 101, 3, 246, 0],\n",
       " [0, 1, 163, 40, 44, 218, 2, 4, 40, 196, 0],\n",
       " [0, 4, 40, 8, 271, 29, 4, 642, 196, 0],\n",
       " [0, 1, 406, 3, 507, 24, 17, 417, 32, 2, 1, 72, 0],\n",
       " [0, 1, 168, 3, 507, 457, 2, 747, 5, 4, 408, 0],\n",
       " [0, 1, 31, 3, 507, 11, 2, 30, 3, 1, 563, 72, 0],\n",
       " [0, 13, 16, 11, 410, 507, 1587, 32, 14, 4, 72, 0],\n",
       " [0, 507, 17, 360, 32, 313, 1, 199, 194, 0],\n",
       " [0, 362, 8, 243, 7, 283, 355, 211, 1, 256, 0],\n",
       " [0, 1, 353, 8, 83, 5, 4, 120, 5, 4, 216, 3, 1, 2289, 99, 0],\n",
       " [0, 1, 27, 271, 29, 1, 244, 222, 6, 156, 0],\n",
       " [0, 1, 353, 8, 225, 1, 25, 607, 2, 1, 230, 0],\n",
       " [0, 1, 453, 3, 1, 353, 618, 1, 1213, 0],\n",
       " [0, 1, 426, 11, 2, 374, 6, 1, 7635, 2161, 2, 26, 0],\n",
       " [0, 137, 8, 1, 166, 3, 1, 426, 2, 30, 3, 1, 374, 0],\n",
       " [0, 1, 111, 426, 8, 11, 2, 4, 2541, 3, 1, 374, 0],\n",
       " [0, 137, 8, 1, 426, 2, 1, 2541, 2, 1, 22, 0],\n",
       " [0, 1, 426, 11, 2, 1, 374, 142, 12, 5312, 3, 1, 130, 0]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences2[50000:50020]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we put a 0 at the beginning and ending of each sentence.\n",
    "\n",
    "## A scaled-down word2vec-like autoencoder\n",
    "\n",
    "We're going to create a feed-forward network that represents an autoencoder similar to the skip-gram model of word2vec --- the word in focus is going to predict its immediate neighbour before and after.  This window is narrower than word2vec, and we are not going to implement negative sampling.\n",
    "\n",
    "So we need to split the text into training samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training(seqs):\n",
    "    collect = []\n",
    "    for seq in seqs:\n",
    "        for i in range(1, len(seq)-1):\n",
    "            collect.append((seq[i], [seq[i-1],seq[i+1]]))\n",
    "    return [x[0] for x in collect], [x[1] for x in collect]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "numtrain_X, numtrain_y = create_training(sequences2[0:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X is the word in question, y0 is the word before it, and y1 is the word after it. That's why we need to pad the sentences with zeros.  Now we can convert these into \"one-hot\" vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = [to_categorical(x, num_classes=10000) for x in numtrain_X]\n",
    "train_y0 = [to_categorical(y[0], num_classes=10000) for y in numtrain_y]\n",
    "train_y1 = [to_categorical(y[1], num_classes=10000) for y in numtrain_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10515, 10515)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_X), len(numtrain_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000,), 10515, 10515)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X[0].shape, len(train_y0), len(train_y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 0., 0., ..., 0., 0., 0.], dtype=float32),\n",
       " array([1., 0., 0., ..., 0., 0., 0.], dtype=float32))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X[0], train_y0[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model design\n",
    "\n",
    "This is an autoencoder, so we are going to compress the input representation to a smaller vector space, here 100-dimensional.  But we need to split the input back into predictors for words in our original vector space.  The prediction is done via softmaxes over dense layers of the right size.  In class we had one dense layer but that does not make sense for two softmaxes, so it's edited here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Model\n",
    "from keras.layers import Input, Dense, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputlayer = Input(shape=(10000,))\n",
    "encoder = Dense(100)(inputlayer)\n",
    "decoder1 = Dense(10000)(encoder)\n",
    "decoder2 = Dense(10000)(encoder)\n",
    "\n",
    "activation1 = Activation('softmax')(decoder1)\n",
    "activation2 = Activation('softmax')(decoder2)\n",
    "\n",
    "model = Model(inputs=[inputlayer], outputs=[activation1, activation2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile('adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 10000)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 100)          1000100     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 10000)        1010000     dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 10000)        1010000     dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 10000)        0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 10000)        0           dense_6[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 3,020,100\n",
      "Trainable params: 3,020,100\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training and vector extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "10515/10515 [==============================] - 7s 675us/step - loss: 13.5965 - activation_3_loss: 6.6427 - activation_4_loss: 6.9538 - activation_3_acc: 0.1718 - activation_4_acc: 0.1153\n",
      "Epoch 2/40\n",
      "10515/10515 [==============================] - 3s 295us/step - loss: 10.3416 - activation_3_loss: 4.9384 - activation_4_loss: 5.4031 - activation_3_acc: 0.2192 - activation_4_acc: 0.1463\n",
      "Epoch 3/40\n",
      "10515/10515 [==============================] - 3s 306us/step - loss: 9.7722 - activation_3_loss: 4.6682 - activation_4_loss: 5.1041 - activation_3_acc: 0.2296 - activation_4_acc: 0.1812\n",
      "Epoch 4/40\n",
      "10515/10515 [==============================] - 3s 310us/step - loss: 9.3292 - activation_3_loss: 4.4552 - activation_4_loss: 4.8740 - activation_3_acc: 0.2462 - activation_4_acc: 0.1921\n",
      "Epoch 5/40\n",
      "10515/10515 [==============================] - 3s 313us/step - loss: 8.9736 - activation_3_loss: 4.2775 - activation_4_loss: 4.6961 - activation_3_acc: 0.2595 - activation_4_acc: 0.2083\n",
      "Epoch 6/40\n",
      "10515/10515 [==============================] - 3s 319us/step - loss: 8.6570 - activation_3_loss: 4.1262 - activation_4_loss: 4.5309 - activation_3_acc: 0.2723 - activation_4_acc: 0.2264\n",
      "Epoch 7/40\n",
      "10515/10515 [==============================] - 3s 321us/step - loss: 8.3691 - activation_3_loss: 3.9906 - activation_4_loss: 4.3785 - activation_3_acc: 0.2864 - activation_4_acc: 0.2403\n",
      "Epoch 8/40\n",
      "10515/10515 [==============================] - 3s 329us/step - loss: 8.1063 - activation_3_loss: 3.8653 - activation_4_loss: 4.2410 - activation_3_acc: 0.3000 - activation_4_acc: 0.2535\n",
      "Epoch 9/40\n",
      "10515/10515 [==============================] - 3s 331us/step - loss: 7.8676 - activation_3_loss: 3.7490 - activation_4_loss: 4.1185 - activation_3_acc: 0.3139 - activation_4_acc: 0.2642\n",
      "Epoch 10/40\n",
      "10515/10515 [==============================] - 3s 331us/step - loss: 7.6511 - activation_3_loss: 3.6405 - activation_4_loss: 4.0105 - activation_3_acc: 0.3240 - activation_4_acc: 0.2762\n",
      "Epoch 11/40\n",
      "10515/10515 [==============================] - 3s 324us/step - loss: 7.4488 - activation_3_loss: 3.5386 - activation_4_loss: 3.9102 - activation_3_acc: 0.3362 - activation_4_acc: 0.2855\n",
      "Epoch 12/40\n",
      "10515/10515 [==============================] - 4s 334us/step - loss: 7.2558 - activation_3_loss: 3.4427 - activation_4_loss: 3.8130 - activation_3_acc: 0.3457 - activation_4_acc: 0.2947\n",
      "Epoch 13/40\n",
      "10515/10515 [==============================] - 4s 336us/step - loss: 7.0732 - activation_3_loss: 3.3524 - activation_4_loss: 3.7209 - activation_3_acc: 0.3504 - activation_4_acc: 0.3044\n",
      "Epoch 14/40\n",
      "10515/10515 [==============================] - 4s 333us/step - loss: 6.8954 - activation_3_loss: 3.2636 - activation_4_loss: 3.6317 - activation_3_acc: 0.3577 - activation_4_acc: 0.3111\n",
      "Epoch 15/40\n",
      "10515/10515 [==============================] - 4s 336us/step - loss: 6.7277 - activation_3_loss: 3.1808 - activation_4_loss: 3.5470 - activation_3_acc: 0.3622 - activation_4_acc: 0.3149\n",
      "Epoch 16/40\n",
      "10515/10515 [==============================] - 4s 344us/step - loss: 6.5667 - activation_3_loss: 3.0997 - activation_4_loss: 3.4670 - activation_3_acc: 0.3686 - activation_4_acc: 0.3175\n",
      "Epoch 17/40\n",
      "10515/10515 [==============================] - 4s 349us/step - loss: 6.4153 - activation_3_loss: 3.0223 - activation_4_loss: 3.3930 - activation_3_acc: 0.3741 - activation_4_acc: 0.3230\n",
      "Epoch 18/40\n",
      "10515/10515 [==============================] - 4s 345us/step - loss: 6.2700 - activation_3_loss: 2.9492 - activation_4_loss: 3.3207 - activation_3_acc: 0.3789 - activation_4_acc: 0.3266\n",
      "Epoch 19/40\n",
      "10515/10515 [==============================] - 4s 345us/step - loss: 6.1356 - activation_3_loss: 2.8813 - activation_4_loss: 3.2543 - activation_3_acc: 0.3862 - activation_4_acc: 0.3291\n",
      "Epoch 20/40\n",
      "10515/10515 [==============================] - 4s 336us/step - loss: 6.0093 - activation_3_loss: 2.8177 - activation_4_loss: 3.1917 - activation_3_acc: 0.3881 - activation_4_acc: 0.3331\n",
      "Epoch 21/40\n",
      "10515/10515 [==============================] - 3s 320us/step - loss: 5.8917 - activation_3_loss: 2.7580 - activation_4_loss: 3.1337 - activation_3_acc: 0.3912 - activation_4_acc: 0.3372\n",
      "Epoch 22/40\n",
      "10515/10515 [==============================] - 3s 330us/step - loss: 5.7799 - activation_3_loss: 2.7025 - activation_4_loss: 3.0774 - activation_3_acc: 0.3984 - activation_4_acc: 0.3412\n",
      "Epoch 23/40\n",
      "10515/10515 [==============================] - 4s 340us/step - loss: 5.6767 - activation_3_loss: 2.6505 - activation_4_loss: 3.0261 - activation_3_acc: 0.4018 - activation_4_acc: 0.3456\n",
      "Epoch 24/40\n",
      "10515/10515 [==============================] - 4s 343us/step - loss: 5.5821 - activation_3_loss: 2.6032 - activation_4_loss: 2.9789 - activation_3_acc: 0.4055 - activation_4_acc: 0.3495\n",
      "Epoch 25/40\n",
      "10515/10515 [==============================] - 4s 340us/step - loss: 5.4945 - activation_3_loss: 2.5593 - activation_4_loss: 2.9352 - activation_3_acc: 0.4082 - activation_4_acc: 0.3536\n",
      "Epoch 26/40\n",
      "10515/10515 [==============================] - 4s 349us/step - loss: 5.4157 - activation_3_loss: 2.5206 - activation_4_loss: 2.8951 - activation_3_acc: 0.4089 - activation_4_acc: 0.3561\n",
      "Epoch 27/40\n",
      "10515/10515 [==============================] - 4s 349us/step - loss: 5.3444 - activation_3_loss: 2.4858 - activation_4_loss: 2.8586 - activation_3_acc: 0.4087 - activation_4_acc: 0.3583\n",
      "Epoch 28/40\n",
      "10515/10515 [==============================] - 4s 349us/step - loss: 5.2804 - activation_3_loss: 2.4542 - activation_4_loss: 2.8262 - activation_3_acc: 0.4090 - activation_4_acc: 0.3585\n",
      "Epoch 29/40\n",
      "10515/10515 [==============================] - 4s 351us/step - loss: 5.2240 - activation_3_loss: 2.4267 - activation_4_loss: 2.7974 - activation_3_acc: 0.4093 - activation_4_acc: 0.3574\n",
      "Epoch 30/40\n",
      "10515/10515 [==============================] - 4s 348us/step - loss: 5.1733 - activation_3_loss: 2.4021 - activation_4_loss: 2.7711 - activation_3_acc: 0.4098 - activation_4_acc: 0.3568\n",
      "Epoch 31/40\n",
      "10515/10515 [==============================] - 4s 363us/step - loss: 5.1285 - activation_3_loss: 2.3808 - activation_4_loss: 2.7477 - activation_3_acc: 0.4071 - activation_4_acc: 0.3577\n",
      "Epoch 32/40\n",
      "10515/10515 [==============================] - 3s 328us/step - loss: 5.0890 - activation_3_loss: 2.3617 - activation_4_loss: 2.7273 - activation_3_acc: 0.4085 - activation_4_acc: 0.3566\n",
      "Epoch 33/40\n",
      "10515/10515 [==============================] - 3s 329us/step - loss: 5.0553 - activation_3_loss: 2.3458 - activation_4_loss: 2.7095 - activation_3_acc: 0.4067 - activation_4_acc: 0.3563\n",
      "Epoch 34/40\n",
      "10515/10515 [==============================] - 4s 360us/step - loss: 5.0257 - activation_3_loss: 2.3323 - activation_4_loss: 2.6933 - activation_3_acc: 0.4067 - activation_4_acc: 0.3572\n",
      "Epoch 35/40\n",
      "10515/10515 [==============================] - 4s 349us/step - loss: 4.9997 - activation_3_loss: 2.3198 - activation_4_loss: 2.6799 - activation_3_acc: 0.4056 - activation_4_acc: 0.3575\n",
      "Epoch 36/40\n",
      "10515/10515 [==============================] - 3s 325us/step - loss: 4.9763 - activation_3_loss: 2.3084 - activation_4_loss: 2.6680 - activation_3_acc: 0.4043 - activation_4_acc: 0.3578\n",
      "Epoch 37/40\n",
      "10515/10515 [==============================] - 4s 363us/step - loss: 4.9555 - activation_3_loss: 2.2995 - activation_4_loss: 2.6560 - activation_3_acc: 0.4040 - activation_4_acc: 0.3576\n",
      "Epoch 38/40\n",
      "10515/10515 [==============================] - 4s 348us/step - loss: 4.9366 - activation_3_loss: 2.2895 - activation_4_loss: 2.6471 - activation_3_acc: 0.4047 - activation_4_acc: 0.3554\n",
      "Epoch 39/40\n",
      "10515/10515 [==============================] - 3s 332us/step - loss: 4.9215 - activation_3_loss: 2.2826 - activation_4_loss: 2.6389 - activation_3_acc: 0.4024 - activation_4_acc: 0.3540\n",
      "Epoch 40/40\n",
      "10515/10515 [==============================] - 3s 321us/step - loss: 4.9074 - activation_3_loss: 2.2763 - activation_4_loss: 2.6310 - activation_3_acc: 0.4021 - activation_4_acc: 0.3560\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc98dd10b38>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([train_X], [train_y0, train_y1], batch_size=40, epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.14314662,  0.16355611, -0.07532723,  0.07635795, -0.41619694,\n",
       "       -0.14242716,  0.1496203 ,  0.07238261,  0.10004503,  0.18769394,\n",
       "        0.12176848, -0.18580864,  0.20597501, -0.11215528,  0.32767844,\n",
       "       -0.145422  , -0.14715248,  0.11516788, -0.1380871 , -0.04156268,\n",
       "        0.12133057,  0.19974759,  0.26606095, -0.20170295, -0.20935528,\n",
       "       -0.12681833,  0.03637717,  0.15068875, -0.24225724, -0.00805199,\n",
       "        0.1123011 ,  0.06966335, -0.23864731,  0.18234386, -0.07011341,\n",
       "        0.01986277, -0.15526071,  0.11711799,  0.05910657, -0.15329146,\n",
       "        0.07250773,  0.24581744,  0.0720281 , -0.076203  , -0.09033831,\n",
       "        0.19663027,  0.2895875 ,  0.10339213,  0.1663343 ,  0.20417655,\n",
       "        0.16058923,  0.16953008,  0.1681371 , -0.26785317,  0.14404684,\n",
       "       -0.23360181, -0.1335926 ,  0.1905494 ,  0.0417267 ,  0.13558243,\n",
       "       -0.01929747, -0.15311107,  0.16830768,  0.2660324 , -0.13602369,\n",
       "        0.17998797,  0.07145264,  0.15956768, -0.19569813,  0.1975249 ,\n",
       "       -0.13934825,  0.24425718, -0.08184303, -0.07715555, -0.18054985,\n",
       "       -0.0759488 ,  0.31397125, -0.069511  , -0.04752418,  0.12797786,\n",
       "       -0.24155715, -0.03401377, -0.15881675,  0.08852226, -0.22006084,\n",
       "       -0.12628688, -0.18996713, -0.08970452,  0.01876661,  0.16303046,\n",
       "        0.17049709, -0.17097403,  0.12833293,  0.03845013, -0.09138246,\n",
       "        0.30698258,  0.10354707,  0.1338715 ,  0.20521551,  0.07713459],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These weights are not meaningful by themselves, for the 100-dimensional layer. We need to apply them to each word input to get the embedding we want.  So we copy the weights to a new network that just has the decoder layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputlayer2 = Input(shape=(10000,))\n",
    "encoder2 = Dense(100)(inputlayer2)\n",
    "\n",
    "model2 = Model(inputs=[inputlayer2], outputs=[encoder2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 10000)             0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 100)               1000100   \n",
      "=================================================================\n",
      "Total params: 1,000,100\n",
      "Trainable params: 1,000,100\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.set_weights(model.get_weights()[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10515"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "limited_vocab = np.unique(train_X, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "limited_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we just did was get the input vocabulary by finding all unique vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1405, 10000)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "limited_vocab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "limited_vocab[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model2.predict(limited_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.18191895,  0.7673938 , -0.42503676,  0.21641074, -0.43395153,\n",
       "       -0.7462448 ,  0.5142629 ,  0.800788  ,  0.57447726,  0.70238596,\n",
       "        0.3525356 ,  0.30519813,  0.4696656 , -0.31868902,  0.12342709,\n",
       "       -0.12800355, -0.3328032 ,  0.50857294, -0.32948518, -0.01759833,\n",
       "        0.70950234, -0.02983892,  0.32943922,  0.10977852, -0.4553824 ,\n",
       "       -0.5262557 , -0.03285315,  0.75082207, -0.13913803,  0.14193931,\n",
       "       -0.0392445 ,  0.3051605 , -0.24927881,  0.2694665 ,  0.28161168,\n",
       "        0.59672505,  0.03770941,  0.7673499 , -0.31700903, -0.24245512,\n",
       "        0.22706467,  0.12413344,  0.01418295, -0.04698663, -0.524775  ,\n",
       "       -0.44773534, -0.29795176, -0.1553868 ,  0.8976282 ,  0.6257448 ,\n",
       "        0.40681547, -0.08134015, -0.2900033 , -0.23900709, -0.00960328,\n",
       "       -0.55789924,  0.0084601 ,  0.5626581 , -0.64054185,  0.52611536,\n",
       "       -0.6000326 , -0.09984165,  0.0429983 ,  0.58272547,  0.08460243,\n",
       "        0.33115682,  0.19802988,  0.4621813 , -0.34791082,  0.93366915,\n",
       "       -0.37109882, -0.37080076, -0.4779321 , -0.11946727, -0.17482957,\n",
       "        0.38843325,  0.5817672 ,  0.49887908, -0.07876313, -0.00296336,\n",
       "       -0.12998395, -0.77928275,  0.44195825,  0.55193084, -0.6302212 ,\n",
       "       -0.3750299 , -0.70445585, -0.3082337 ,  0.20623311,  0.8093503 ,\n",
       "        0.03599064, -0.36930373, -0.26060772,  0.00306319, -0.3145823 ,\n",
       "        0.23230994, -0.7129228 ,  0.95123625,  0.0687189 ,  0.26063088],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.25502354,  0.04944277, -0.5670153 , -0.29917794, -0.69977474,\n",
       "       -0.66997534,  0.28384385,  0.42144537,  0.04923905,  0.21359417,\n",
       "       -0.21635136, -0.59145594, -0.1079127 , -0.548488  ,  0.74762267,\n",
       "       -0.6050516 ,  0.17679739,  0.60143137, -0.63196814, -0.44900575,\n",
       "        0.18229765,  0.36886823,  0.5242398 , -0.5135215 , -0.68832326,\n",
       "        0.22165588, -0.4248374 , -0.31730098, -0.6920254 ,  0.0761136 ,\n",
       "        0.5636907 ,  0.5056648 , -0.697595  , -0.28111702, -0.5325645 ,\n",
       "       -0.38215536,  0.22438112,  0.08300776,  0.33157372, -0.6050271 ,\n",
       "       -0.258968  ,  0.6488949 ,  0.58741826, -0.2831214 ,  0.30824584,\n",
       "        0.61425304,  0.7610241 ,  0.5610622 , -0.16325665,  0.6506476 ,\n",
       "        0.1443589 ,  0.6208735 ,  0.06706424, -0.67102075,  0.33918086,\n",
       "        0.13869268, -0.55294687,  0.6399654 , -0.38942868,  0.28037673,\n",
       "        0.49838746,  0.10069683, -0.11020742,  0.7295141 , -0.5611611 ,\n",
       "        0.4756082 , -0.01054832, -0.17739466, -0.44328785,  0.61823744,\n",
       "        0.22767143,  0.69527555, -0.02273235, -0.48083994, -0.65500164,\n",
       "       -0.4239681 , -0.1428701 , -0.56971633, -0.21250376,  0.5748743 ,\n",
       "       -0.69639754, -0.49888328,  0.24966991, -0.32849535,  0.07712786,\n",
       "        0.01056685,  0.1840778 ,  0.37559402,  0.43100902,  0.18964678,\n",
       "        0.58898616,  0.14757304,  0.5698422 ,  0.53869957, -0.03344442,\n",
       "        0.7317958 , -0.30861524,  0.5197878 , -0.22992653, -0.2692897 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every vocabulary item has a different 100-dimensional vector output from the model now. They can be used as embeddings for other tasks, such as clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM-based autoencoder for sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for illustration purposes, we take a small subset of the sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "smallseqs = sequences[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, LSTM, TimeDistributed, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to learn vectors from sequences, the sequences need to have the same length (because we can't multiply matrices with variable sizes, and all of this is just fancy matrix multiplication)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "paddedseqs = pad_sequences(smallseqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 36)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paddedseqs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create the categorical one-hot vectors for each sequence, because this is what we predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "catseqs = to_categorical(paddedseqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 36, 9425)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catseqs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is small hack to make sure we got the right vocab size.  The dimensionality of the categorical vectors is always one more than the vocab size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9424"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "dim = max(np.unique(paddedseqs))\n",
    "dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqlayer = Input(shape=(36,))\n",
    "emblayer = Embedding(9425, 100, input_length=36)(seqlayer)\n",
    "lstm1 = LSTM(100, return_sequences=True)(emblayer)\n",
    "dropoutlayer = Dropout(0.1)(lstm1)\n",
    "lstm2 = LSTM(100, return_sequences=True)(dropoutlayer)\n",
    "tdlayer = TimeDistributed(Dense(9425))(lstm2)\n",
    "softmaxlayer = Activation('softmax')(tdlayer)\n",
    "\n",
    "model = Model(inputs=[seqlayer], outputs=[softmaxlayer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the Embeddings layer to learn word embeddings this time. It only needs to know the maximum length of the sentences (36).  We'll get 100-dimensional vectors from it.  It does the rest of the work.  Then we have a couple of LSTM layers with dropout between them (again, what dropout is good is an empirical question/matter of judgement).  We then need to predict the sequence, which is what the TimeDistributed layer does -- it repeats a vocab-sized Dense layer over the length of the sequence.  And we predict the *current* word via softmax.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 36)                0         \n",
      "_________________________________________________________________\n",
      "embedding_5 (Embedding)      (None, 36, 100)           942500    \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 36, 100)           80400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 36, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 36, 100)           80400     \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 36, 9425)          951925    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 36, 9425)          0         \n",
      "=================================================================\n",
      "Total params: 2,055,225\n",
      "Trainable params: 2,055,225\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile('rmsprop', loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when we train the model, we give the padded integer indices as input for the Embeddings layer, but as output we give it the categorical vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss: 5.2073 - acc: 0.6848\n",
      "Epoch 2/30\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 2.5187 - acc: 0.7079A: 1s - loss: 2.5715 - a\n",
      "Epoch 3/30\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 2.2848 - acc: 0.7079\n",
      "Epoch 4/30\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 2.2282 - acc: 0.7079\n",
      "Epoch 5/30\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 2.2227 - acc: 0.7079\n",
      "Epoch 6/30\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 2.2213 - acc: 0.7079\n",
      "Epoch 7/30\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 2.2196 - acc: 0.7079\n",
      "Epoch 8/30\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 2.2199 - acc: 0.7079\n",
      "Epoch 9/30\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 2.2200 - acc: 0.7079\n",
      "Epoch 10/30\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 2.2198 - acc: 0.7079\n",
      "Epoch 11/30\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 2.2201 - acc: 0.7079\n",
      "Epoch 12/30\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 2.2203 - acc: 0.7079\n",
      "Epoch 13/30\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 2.2202 - acc: 0.7079\n",
      "Epoch 14/30\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 2.2201 - acc: 0.7079\n",
      "Epoch 15/30\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 2.2200 - acc: 0.7079\n",
      "Epoch 16/30\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 2.2198 - acc: 0.7079\n",
      "Epoch 17/30\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 2.2090 - acc: 0.7079\n",
      "Epoch 18/30\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 2.1853 - acc: 0.7079\n",
      "Epoch 19/30\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 2.1639 - acc: 0.7079\n",
      "Epoch 20/30\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 2.1471 - acc: 0.7079\n",
      "Epoch 21/30\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 2.1304 - acc: 0.7079\n",
      "Epoch 22/30\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 2.1146 - acc: 0.7079\n",
      "Epoch 23/30\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 2.0394 - acc: 0.7079\n",
      "Epoch 24/30\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 1.6831 - acc: 0.7193\n",
      "Epoch 25/30\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 1.5508 - acc: 0.7527\n",
      "Epoch 26/30\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 1.4676 - acc: 0.7528\n",
      "Epoch 27/30\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 1.3995 - acc: 0.7541\n",
      "Epoch 28/30\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 1.3424 - acc: 0.7586\n",
      "Epoch 29/30\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 1.3033 - acc: 0.7594\n",
      "Epoch 30/30\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 1.2716 - acc: 0.7607\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7feaa7bdc2e8>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([paddedseqs], [catseqs], epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convergence happens very quickly because our data is super small...but wait, after we get past the 30th epoch it suddenly takes off! (Try this without dropout.) Sometimes it pays to wait a bit.  We can re-run fit on the current weights repeatedly and keep training it until it really converges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract weights from the embeddings layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 36)                0         \n",
      "_________________________________________________________________\n",
      "embedding_5 (Embedding)      (None, 36, 100)           942500    \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 36, 100)           80400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 36, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 36, 100)           80400     \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 36, 9425)          951925    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 36, 9425)          0         \n",
      "=================================================================\n",
      "Total params: 2,055,225\n",
      "Trainable params: 2,055,225\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.08595851, -0.1348605 , -0.05951789, -0.09311055, -0.04006767,\n",
       "       -0.13085993, -0.17875338, -0.12385153,  0.1233968 ,  0.07179775,\n",
       "       -0.04407612, -0.04639833,  0.16350667,  0.15804137, -0.16682605,\n",
       "        0.07753159, -0.14385594,  0.04069513, -0.07771698,  0.1258528 ,\n",
       "        0.05372755,  0.07482429, -0.04459634, -0.08102922,  0.09292468,\n",
       "        0.18648   ,  0.15452106, -0.07762194,  0.12481114,  0.10383981,\n",
       "       -0.04829976,  0.17528687,  0.05951104,  0.05654939, -0.08203832,\n",
       "       -0.17533015,  0.08978003, -0.04999898, -0.16869149,  0.11418618,\n",
       "        0.12412349, -0.07800698, -0.15448354, -0.05552231,  0.0884326 ,\n",
       "       -0.13845249,  0.0678155 , -0.15825325, -0.08738711, -0.11881547,\n",
       "        0.10172014,  0.10572896, -0.13061467,  0.04477144,  0.04484133,\n",
       "       -0.0725379 ,  0.11765544,  0.12547421,  0.1331902 , -0.15013619,\n",
       "       -0.13198432, -0.0692712 , -0.17415167, -0.08659714,  0.10983732,\n",
       "        0.07686424,  0.03690993,  0.09399714,  0.14904135,  0.12114418,\n",
       "       -0.11326628,  0.16355154,  0.07218754,  0.16558218,  0.11522987,\n",
       "        0.04829931, -0.13099463, -0.1536287 ,  0.06698971,  0.13605493,\n",
       "        0.04097982, -0.05482944, -0.10535657,  0.15789983, -0.07703776,\n",
       "       -0.11625398, -0.17679292,  0.07850827,  0.08247884,  0.0551442 ,\n",
       "       -0.07875291, -0.14583479, -0.16926315, -0.12770866, -0.15556413,\n",
       "       -0.11537436, -0.10692012, -0.06475236,  0.13136941,  0.0783727 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00440912, -0.04126433,  0.02153069,  0.01764884,  0.02494913,\n",
       "       -0.00458091, -0.02202054, -0.02562576, -0.01499826,  0.02869104,\n",
       "       -0.02300212,  0.02751501, -0.03722743,  0.00067579, -0.03031388,\n",
       "        0.03184034,  0.00276793, -0.0089811 , -0.04803849,  0.04574609,\n",
       "       -0.01663978,  0.00677484,  0.02859687,  0.01374232,  0.01842347,\n",
       "       -0.00318735, -0.01623718, -0.00460901, -0.02634054,  0.04690667,\n",
       "       -0.01026788,  0.02768691, -0.02393703,  0.00526467, -0.02245242,\n",
       "        0.01438989, -0.01939533,  0.02299419, -0.01693181, -0.02457434,\n",
       "        0.02575289, -0.0399997 , -0.0349864 , -0.01022828, -0.01965418,\n",
       "       -0.04237857,  0.02159353,  0.01240896,  0.02745398, -0.04243142,\n",
       "        0.04358572,  0.03999844, -0.04961624,  0.04602572,  0.05444944,\n",
       "       -0.02967173, -0.01196242,  0.03462901,  0.04673612,  0.01505011,\n",
       "        0.0179581 , -0.0021507 ,  0.0106782 , -0.04382759, -0.00679799,\n",
       "        0.02960026,  0.00406672,  0.02004926, -0.02685613,  0.03648892,\n",
       "       -0.00311331,  0.02781168,  0.02685033, -0.01157164,  0.02295201,\n",
       "        0.01805446, -0.01294051, -0.0381107 ,  0.03365441, -0.01906772,\n",
       "        0.01290233,  0.03752738, -0.04388161,  0.02295126,  0.01650335,\n",
       "       -0.0397476 , -0.0118693 ,  0.05979265, -0.02547476, -0.01631382,\n",
       "        0.03597668,  0.03355418, -0.01618367,  0.01101474, -0.05668452,\n",
       "        0.01560499, -0.05651992, -0.0193808 ,  0.02248194,  0.00496483],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()[0][400]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 0th layer of the model contains the weights for the Embedding layer (the Input layer doesn't *have* weights).  The integer word indices are an index into the corresponding embedding in the Embedding layer.  This way, you can get the word vectors out and cluster, etc, as before, or use them to train another model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
